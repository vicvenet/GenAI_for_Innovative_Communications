{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383e83bd",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vicvenet/GenAI_for_Innovative_Communications/blob/main/2025_S1/Week_7/sentiment_analysis_distilbert_finetune.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff187a9c",
   "metadata": {},
   "source": [
    "# Objectives of todayâ€™s workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a2978",
   "metadata": {},
   "source": [
    "Understanding how to:\n",
    "1. Identify a suitable pre-trained GenAI model\n",
    "2. Adjust that model with a light-weight and inexpensive approach to achieve this objective by fine-tuning with Low-Rank Adaptation\n",
    "3. Apply the fine-tuned model to a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa29408",
   "metadata": {},
   "source": [
    "# 1. Identify a suitable pre-trained GenAI model \n",
    "\n",
    "a. Consider the actual use case and the data available\n",
    "\n",
    "In the current hypthetical context of a sentiment analysis task for a company that wants to monitor the sentiment of their customers, we need a model that can run fast at a low cost and probably need to identify basic emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a111997",
   "metadata": {},
   "source": [
    "b. Do not reinvent the wheel: Aim to use data that you already have or use an open-source dataset and start with a pre-trained model that fits the actual use case and can be fine-tuned with the available data\n",
    "\n",
    "While models with multiple billion parameters are available, companies are more likely to use smaller models that are more suitable for the task and can be fine-tuned with the available data:\n",
    "- DistilBERT Base uncased which is a distilled version of the popular BERT model that is known to be good at sentiment analysis is a good candidate for this task\n",
    "- It is very small (66M parameters) so it can run fast\n",
    "- Even though it is designed for sequence to sequence generation, it can be adjusted to a classification task by replacing the final layer with a new one that is trained for the specific classification task. This is what we will do in this workshop.\n",
    "\n",
    "Regarding the dataset to fine-tune the model on, there are many available datasets for sentiment analysis,  and we will use a subset of the Go Emotions dataset (https://huggingface.co/datasets/google-research-datasets/go_emotions) which is a collection of emotion-based Reddit comments with 27 different emotions. In real life, the company will have their own dataset of customer feedback and it will use to fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b621cc5",
   "metadata": {},
   "source": [
    "c. Size matters: Aim to use a model that is not too big, so that it can be used at scale at a reasonable cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96d09f",
   "metadata": {},
   "source": [
    "# 2. Adjust that model with a light-weight and inexpensive approach to achieve this objective by fine-tuning with Low-Rank Adaptation\n",
    "\n",
    "a. Use a light-weight and inexpensive approach to fine-tune the model by using Low-Rank Adaptation (LoRA)\n",
    "\n",
    "b. LoRA is a technique that allows us to fine-tune the model with a smaller number of trainable parameters, which is more efficient and easier to manage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc80715",
   "metadata": {},
   "source": [
    "Install the libraries with specific versions for reproducibility (Google Colab already has Pytorch installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d225b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "    transformers>=4.30.0 \\\n",
    "    datasets>=2.12.0 \\\n",
    "    peft>=0.4.0 \\\n",
    "    tqdm>=4.65.0 \\\n",
    "    scikit-learn>=1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd8e60",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701f27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AdamW\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34248c9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Directory Setup\n",
    "Create necessary directories for saving model, dataset, and tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a9b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"saved_data\")\n",
    "MODEL_DIR = SAVE_DIR / \"model\"\n",
    "DATASET_DIR = SAVE_DIR / \"dataset\"\n",
    "TOKENIZED_DIR = SAVE_DIR / \"tokenized_dataset\"\n",
    "LORA_DIR = MODEL_DIR / \"trained_LoRA\"  # New directory for LoRA adapters\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(TOKENIZED_DIR, exist_ok=True)\n",
    "os.makedirs(LORA_DIR, exist_ok=True)  # Create LoRA directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73770740",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "Load the Go Emotions dataset with the Hugging Face dataset library, using cached version if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217d3f0b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Check if dataset is already downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf0616",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = DATASET_DIR / \"go_emotions_simplified\"\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Loading cached dataset...\")\n",
    "    dataset = load_dataset(\"go_emotions\", \"simplified\", \n",
    "                         cache_dir=str(dataset_path))\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "    dataset.save_to_disk(str(dataset_path))\n",
    "\n",
    "# Get number of unique labels from the dataset\n",
    "num_labels = len(set(\n",
    "    label \n",
    "    for example in dataset['train'] \n",
    "    for label in example['labels']\n",
    "))\n",
    "print(f\"Number of unique labels in dataset: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab551e",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "Initialize the DistilBERT model and tokenizer using the Hugging Face library transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d281146",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d361e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12a6bd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Initialize or load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = LORA_DIR / \"distilbert_lora_go_emotions\"\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading saved LoRA adapter...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        str(model_path),\n",
    "        num_labels=num_labels  # Ensure consistent number of labels\n",
    "    )\n",
    "else:\n",
    "    print(\"Initializing new model...\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=num_labels  # Use number of labels from dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ff6304",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "Configure Low-Rank Adaptation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a2095",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Configure LoRA with target modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad8d35",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Apply LoRa to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1384c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504503ff",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "Define tokenization function and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc0da1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    tokenized = tokenizer(\n",
    "        example['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,  # Set maximum sequence length\n",
    "        # 64 is safe as the longest sentence in the dataset is 33 words \n",
    "        # which is most likely less than 64 tokens\n",
    "        return_tensors=None  # Don't return tensors yet\n",
    "    )\n",
    "    # For batched processing, labels will be a list of lists\n",
    "    if isinstance(example['labels'], list) and isinstance(example['labels'][0], list):\n",
    "        # Take first label for each example in batch\n",
    "        tokenized['labels'] = [labels[0] if labels else 0 for labels in example['labels']]\n",
    "    else:\n",
    "        # Single example case\n",
    "        tokenized['labels'] = example['labels'][0] if example['labels'] else 0\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62602d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dfe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb995c",
   "metadata": {},
   "source": [
    "## Dataset Tokenization\n",
    "Tokenize the dataset and prepare it for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b7752",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Check if tokenized dataset exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18884520",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_path = TOKENIZED_DIR / \"tokenized_dataset\"\n",
    "if os.path.exists(tokenized_path):\n",
    "    print(\"Loading cached tokenized dataset...\")\n",
    "    tokenized_dataset = dataset.load_from_disk(str(tokenized_path))\n",
    "else:\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=batch_size * 4,  # Process 4 training batches at once for efficiency\n",
    "        remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    # Set format for PyTorch\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\n",
    "        \"torch\", \n",
    "        columns=[\n",
    "            \"input_ids\", \n",
    "            \"attention_mask\", \n",
    "            \"labels\"\n",
    "        ]\n",
    "    )\n",
    "    tokenized_dataset.save_to_disk(str(tokenized_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29b90a",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "Create DataLoaders for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe597a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Create DataLoader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37c4ab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcad42c",
   "metadata": {},
   "source": [
    "Create DataLoader for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725fe3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    tokenized_dataset[\"validation\"],\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3408aa7f",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "Configure device and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b873505",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
    "\n",
    "# Training constants\n",
    "max_grad_norm = 1.0\n",
    "num_epochs = 3\n",
    "\n",
    "# Calculate number of training steps\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = num_training_steps // 10  # 10% of total steps for warmup\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e991a0",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Train the model for the specified number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83833f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        current_loss = epoch_loss / (progress_bar.n + 1)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": f\"{current_loss:.4f}\",\n",
    "            \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "    \n",
    "    # Print epoch summary\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"\\nEpoch {epoch+1} - Average Loss: {avg_epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b4eca",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the model on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9693b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f9254",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82a4c37",
   "metadata": {},
   "source": [
    "# 3. Apply the fine-tuned model to a simple example\n",
    "Try the model on a sample sentence to verify it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion mapping (before predict_sentiment function)\n",
    "EMOTIONS = [\n",
    "    \"admiration\", \"amusement\", \"anger\",\n",
    "    \"annoyance\", \"approval\", \"caring\",\n",
    "    \"confusion\", \"curiosity\", \"desire\",\n",
    "    \"disappointment\", \"disapproval\",\n",
    "    \"disgust\", \"embarrassment\",\n",
    "    \"excitement\", \"fear\", \"gratitude\",\n",
    "    \"grief\", \"joy\", \"love\", \"nervousness\",\n",
    "    \"neutral\", \"optimism\", \"pride\",\n",
    "    \"realization\", \"relief\", \"remorse\",\n",
    "    \"sadness\", \"surprise\"\n",
    "]\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Map numerical prediction to emotion name\n",
    "    emotion = EMOTIONS[prediction.item()]\n",
    "    return emotion\n",
    "\n",
    "# Test with an example sentence\n",
    "example_text = \"I feel excited to learn how to use GenAI!\"\n",
    "prediction = predict_sentiment(example_text)\n",
    "print(\"\\nExample Prediction:\")\n",
    "print(f\"Text: {example_text}\")\n",
    "print(f\"Predicted emotion: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d52dac",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "Save the trained LoRA adapter and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0834e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter and tokenizer\n",
    "print(\"Saving LoRA adapter and tokenizer...\")\n",
    "model.save_pretrained(str(LORA_DIR / \"distilbert_lora_go_emotions\"))\n",
    "tokenizer.save_pretrained(str(LORA_DIR / \"distilbert_lora_go_emotions\"))\n",
    "print(\"Training complete! LoRA adapter and tokenizer saved.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
